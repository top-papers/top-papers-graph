# Туториал проекта SciReason: что делать каждому участнику (очень пошагово)

Этот документ — “инструкция к настольной игре” для проекта на 200–300 человек.

---

## 0) Как устроен проект в двух словах

У нас есть “оркестр” модулей:

1) **Поиск** статей (OpenAlex / arXiv / Semantic Scholar)  
2) **Чтение**: PDF → структурный текст (GROBID)  
3) **Извлечение** фактов: триплеты (S–P–O)  
4) **Граф знаний**: Neo4j (связи)  
5) **Векторный поиск**: Qdrant (контекст)  
6) **Reasoning / Debate**: Энтузиаст ↔ Скептик (LangGraph/цикл)  
7) **Оценка**: тесты + метрики (pytest + Ragas/DeepEval)

Эксперты улучшают пункты 3–7 не “лейблами”, а **управлением логикой**:  
- как правильно делать выводы,
- какие связи ложные,
- какие гипотезы научно слабые.

---

## 1) Онбординг (один раз для всех)

### Шаг 1. Установите базовые инструменты
- Git (любой клиент)
- Docker + Docker Compose
- Python 3.11+

### Шаг 2. Запуск инфраструктуры (5 минут)
1) Откройте терминал в папке репозитория.
2) Создайте `.env`:
   - скопируйте `.env.example` → `.env`
   - при желании заполните ключи LLM (можно позже)
3) Поднимите сервисы:
```bash
docker compose up -d
```

### Шаг 3. Установка Python-зависимостей
```bash
pip install -e ".[dev]"
```

### Шаг 4. Проверка
```bash
top-papers-graph doctor
```

Если всё ок — вы готовы.

---

## 2) “Как мы работаем” — правила взаимодействия

### Правило A. Любая работа = артефакт
- Эксперты → файлы в `data/experts/...`
- Инженеры → код + тесты + (если нужно) миграции/скрипты

### Правило B. Любое улучшение должно быть измеримо
- Добавили новый промпт → добавили тест/метрику или прогнали evaluation
- Изменили пайплайн → не сломали старые результаты (регрессия)

### Правило C. Никаких “магических” знаний
Если связь “очевидна эксперту” — мы фиксируем её как **added_edges** с объяснением, чтобы потом это можно было обучить/проверить.

---

## 3) Пошагово для Эксперта

### 3.1 Выберите домен и “микротему”
1) Найдите свой Squad (или создайте).
2) Выберите микротему на 2 недели:
   - узко, измеримо, с понятными ключевыми словами
   - пример: “катализатор X в реакции Y при условии Z”
3) Согласуйте микротему с Scientific Lead домена.

### 3.2 Проект 1 — Reasoning Trajectories (золотые стандарты)

**Цель:** дать системе эталон научного рассуждения.

Шаги:
1) Возьмите известное открытие/вывод в вашей области.
2) Определите *cutoff date*: какие статьи были доступны до открытия.
3) Подберите 5–10 статей (ID: DOI/arXiv/OpenAlex/S2).
4) Создайте файл на основе шаблона:
   - `data/experts/trajectories/_template.yaml`
   - сохраните как `data/experts/trajectories/YYYY-MM-DD_<domain>_<short>.yaml`
5) Заполните reasoning_chain (6–15 шагов):
   - в каждом шаге: **цитата** → интерпретация → логика → следующий вопрос
6) Сделайте PR:
   - “Add reasoning trajectory: …”
7) В PR обязательно:
   - 1–2 абзаца, зачем этот кейс важен
   - перечисление source_id статей

Что получится:
- это станет “few-shot” примерами и тестовыми кейсами.

### 3.3 Проект 2 — Graph Verification (проверка графов)

**Цель:** убрать галлюцинации связей и научить систему каузальности.

Шаги:
1) Инженер Squad строит граф по подборке статей (см. раздел для инженера).
2) Вам дают “выгрузку” ребер или визуализацию.
3) Вы:
   - подтверждаете сильные связи (**accepted_edges**)
   - режете ложные (**rejected_edges**) с причинами
   - добавляете “имплицитные” (**added_edges**) с объяснением
4) Создаёте файл:
   - `data/experts/graph_reviews/YYYY-MM-DD_<domain>_<graph_id>.json`
5) Делаете PR.

Советы:
- Особенно внимательно к связям “коррелирует” vs “влияет”.
- Если связь “условная” — лучше пометить как слабую/нуждающуюся в данных.

### 3.4 Проект 3 — Hypothesis Red Teaming (рецензирование гипотез)

**Цель:** научить систему предлагать *проверяемые* идеи.

Шаги:
1) Инженер генерирует гипотезы через `top-papers-graph debate ...`
2) Вы выбираете одну гипотезу и пишете рецензию:
   - оценки (0–5) по новизне, обоснованности, проверяемости
   - major/minor issues
   - “что изменит моё мнение” (какие данные нужны)
3) Файл:
   - `data/experts/hypothesis_reviews/YYYY-MM-DD_<domain>_<hyp_id>.json`
4) PR.

---

## 4) Пошагово для Инженера / ML

### 4.1 Сценарий “поднять пайплайн и собрать первый датасет”

#### Шаг 1. Найти статьи
Пример (arXiv):
```bash
top-papers-graph fetch --source arxiv --query "all:graph rag" --limit 10 --out data/papers/search.json
```

OpenAlex:
```bash
top-papers-graph fetch --source openalex --query "graph rag scientific discovery" --limit 10
```

#### Шаг 2. Скачать PDF
MVP: вручную скачайте 1–3 PDF в `data/raw_pdfs/` и создайте meta.json на статью.

*Почему пока так:* скачивание “в одну кнопку” сильно зависит от источника. Мы добавим коннектор позже.

#### Шаг 3. Парсинг PDF (GROBID)
```bash
top-papers-graph parse --pdf data/raw_pdfs/<paper>.pdf --meta configs/meta_example.json --out-dir data/papers/parsed
```

#### Шаг 4. Построить KG + эмбеддинги
```bash
top-papers-graph build-kg --paper-dir data/papers/parsed/<paper_id> --collection demo --domain "Science"
```

Проверка:
- Qdrant: http://localhost:6333/dashboard
- Neo4j: http://localhost:7474

#### Шаг 5. GraphRAG + Debate
```bash
top-papers-graph debate --query "..." --collection demo --domain "Science" --k 8 --max-rounds 3
```

---

### 4.2 Как интегрировать обратную связь экспертов в код

**Graph Reviews → веса ребер:**
1) Читаем `data/experts/graph_reviews/*.json`
2) Для accepted_edges повышаем confidence
3) Для rejected_edges понижаем/удаляем ребро
4) Для added_edges добавляем ребро с “источником: эксперт”

**Hypothesis Reviews → промпт Скептика:**
1) Собираем типовые major issues
2) Превращаем в чек-лист (правила)
3) Добавляем в system prompt Скептика

**Reasoning Trajectories → evals:**
1) Из YAML делаем “вопрос → ожидаемая цепочка шагов”
2) Проверяем, что агент:
   - ссылается на источники,
   - не перескакивает через шаги,
   - не путает причинность.

---

## 5) Интеграция Telegram-бота top-papers-bot (как элемент проекта)

Бот умеет искать статьи и подписывать пользователей на обновления, включая “умный” ранжирующий шаг LLM. citeturn5view0

В этом репозитории он лежит в `third_party/top-papers-bot/` (оригинальный код).  
Рекомендуемый сценарий интеграции:

1) **Бот остаётся “входным шлюзом”**: эксперты/участники ищут и сохраняют подборки.
2) Добавляем “экспорт в SciReason”: после поиска бот может отправить JSON результатов в наш ingestion API (планируется как следующий шаг).
3) SciReason ingestion:
   - скачивание PDF (если доступно),
   - GROBID,
   - KG/Qdrant,
   - дальше — граф/дебаты/оценка.

---

## 6) Дорожная карта на 3 месяца (по-недельному)

### Месяц 1 — Baseline
Недели 1–2:
- Инженеры: стабильный ingestion (PDF→chunks→Qdrant), простая версия KG
- Эксперты: 1 траектория на Squad (минимум), фиксируем типовые ошибки системы

Недели 3–4:
- Инженеры: триплеты + Neo4j, первые graph exports
- Эксперты: первые graph reviews (мин. 1 на Squad)

### Месяц 2 — Feedback loop
Недели 5–6:
- Инженеры: GraphRAG запросы + транзитивность (пути A→C через B)
- Эксперты: масштабируем graph reviews, создаём “глоссарий терминов”

Недели 7–8:
- Инженеры: дебаты агентов (LangGraph) + UI
- Эксперты: первые hypothesis reviews, формируем чек-лист критика

### Месяц 3 — Синтез
Недели 9–10:
- Инженеры: evaluation framework (pytest + ragas/deepeval)
- Эксперты: массовый red teaming гипотез

Недели 11–12:
- Пакуем open-source релиз, пишем meta-paper о методологии
- Выбираем 1–3 “самых сильных” гипотезы и план эксперимента

---

## 7) Где смотреть прогресс
- `pytest -q` — быстрый контроль качества
- Streamlit UI: `streamlit run -m scireason.ui.streamlit_app`
- Neo4j Browser: http://localhost:7474
- Qdrant dashboard: http://localhost:6333/dashboard

---

Если хотите, я могу дополнить репозиторий:
- ingestion “в один клик” из arXiv/OpenAlex,
- экспорт/импорт из top-papers-bot через webhooks,
- автогенерация тестов из reasoning trajectories.
